steps from Hadoop Setup

sudo apt-get update

#sudo apt-get install default-jdk
(hadoop recommmend)
sudo apt install -y openjdk-8-jdk wget

sudo apt-get install rsync

sudo apt-get install ssh

ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa


cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorizedkeys

cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys


wget http://apache.mirrors.tds.net/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz


ls

sudo tar -xvf hadoop-3.2.0.tar.gz


sudo mv hadoop-3.2.0 /usr/local/hadoop

update-alternatives --config java
( /usr/lib/jvm/java-11-openjdk-amd64/bin/java)  JDK path


ssh localhost (System config)

(Text Editor)
sudo apt-get install vim
or
sudo apt-get  install gedit

sudo vim . .bashrc
or
sudo nano . .bashrc


# -- HADOOP ENVIRONMENT VARIABLES START -- #
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
# -- HADOOP ENVIRONMENT VARIABLES END -- #

(run ~/.bashrc)
. .bashrc

(copy)
sudo cp mapred-site.xml.template mapred-site.xml
or
sudo cp mapred-site.xml mapred-site.xml.backup

(open)
sudo nano mapred-site.xml
<configuration>
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
<property>
<name>yarn.app.mapreduce.am.env</name>
<value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>
<property>
<name>mapreduce.map.env</name>
<value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>
<property>
<name>mapreduce.reduce.env</name>
<value>HADOOP_MAPRED_HOME=${HADOOP_HOME}</value>
</property>
</configuration>

(open)
sudo nano yarn-site.xml

<configuration>
<property>
<name>yarn.nodemanager.aux-service</name>
<value>mapreduce_shuffle</value>
</property>
</configuration>


(open)
sudo nano core-site.xml

(hdfs file system uses default name, and  portno 9000)
<configuration>
<property>
<name>fs.default.name</name>
<value>hdfs://localhost:9000/</value>
</property>
</configuration>

(open)

sudo nano hdfs-site.xml

<configuration>
#DFS file Replication time 1
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
#dfs Name node
<property>
<name>dfs.name.dir</name>
<value>file://home/Akbar/hadoopspace/hdfs/namenode</value>
</property>
#dfs Data node
<property>
<name></name>
<value>file://home/Akbar/hadoopspace/hdfs/namenode/datanode</value>
</property>
</configuration>

(open)
sudo nano hadoop-env.sh

export JAVA_HOME="/usr/lib/jvm/java-8-openjdk-amd64"
export JAVA_LIBRARY_PATH=${JAVA_LIBRARY_PATH}:/etc/opencv/lib
save and exit


cd(local user dir)

pwd(check Path dir)

mkdir -p /home/Akbar/hadoopspace/hdfs/namenode

mkdir -p /home/Akbar/hadoopspace/hdfs/namenode/datanode

(Give admin permission)
sudo chown -R Akbar:Akbar /usr/local/hadoop


(To check hadoop is install)
hadoop

(format namenode before start)
hdfs namenode -format

(start all hdfs service)
start-all.sh

(Check it running)
jps

(to stop all service)
stop-all.sh



























